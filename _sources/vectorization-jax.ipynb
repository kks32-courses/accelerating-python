{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "27D3XsP9huyG"
      },
      "source": [
        "# Vectorization and JAX\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/accelerating-python/blob/main/exercise/vectorization-jax-exercise.ipynb)\n",
        "[![Try on DesignSafe](https://raw.githubusercontent.com/geoelements/LearnMPM/main/DesignSafe-Badge.svg)](https://jupyter.designsafe-ci.org/user/name/notebooks/CommunityData/Training/Webinar-Accelerating-Python/vectorization-jax-exercise.ipynb)\n",
        "\n",
        "Let's now generate a following function:\n",
        "\n",
        "$$\\text{signal} = \\sin(2 \\pi f t)$$\n",
        "\n",
        "where, $\\text{signal}$ represents the output signal, $\\sin$ denotes the sine function, $\\pi$ is the mathematical constant Pi (approximately 3.14159), $f$ represents the frequency, and $t$ is the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PBr-PtSucL_N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# time step in seconds\n",
        "dt = 0.005\n",
        "\n",
        "# Frequency in Hz\n",
        "f = 2.0\n",
        "\n",
        "# Number of time steps\n",
        "N = 10000000\n",
        "\n",
        "# Numpy array containing time vector consisting of 32 bit floating point precision values\n",
        "t = np.linspace(0, N * dt, N, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fx0o4AZh25v"
      },
      "source": [
        "## Non-vectorized for-loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et4epTukc385",
        "outputId": "4f456d1b-ce82-457c-d16c-fb02b855bc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorized Python loop time: 39.63473 seconds\n"
          ]
        }
      ],
      "source": [
        "# Non-vectorized operation that uses a Python for loop\n",
        "signal2 = np.empty(N)\n",
        "loop_start_time = time.time()\n",
        "for i in np.arange(N):\n",
        "    signal2[i] = np.sin(2.0 * np.pi * f * t[i])\n",
        "loop_end_time = time.time()\n",
        "print(f\"Vectorized Python loop time: {loop_end_time - loop_start_time:.5f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZfXH69nh4fu"
      },
      "source": [
        "## Vectorized numpy\n",
        "\n",
        "### What is vectorization?\n",
        "\n",
        "Vectorization is a technique used in computer programming and numerical computing to improve the performance of code by applying operations to entire arrays or data structures instead of processing individual elements. In other words, vectorization enables the simultaneous processing of multiple data elements, which can lead to significant speedup in execution time.\n",
        "\n",
        "This approach takes advantage of Single Instruction, Multiple Data (SIMD) hardware features present in modern processors, such as CPUs and GPUs. SIMD allows a single instruction to operate on multiple data elements concurrently, improving the overall throughput of the computation.\n",
        "\n",
        "In the context of numerical computing, vectorization is commonly used in libraries like NumPy, JAX, and TensorFlow, where operations are applied to entire arrays or tensors at once. This eliminates the need for explicit loops in the code and allows the underlying libraries to optimize the computation using SIMD instructions or parallel execution on GPUs and TPUs.\n",
        "\n",
        "![Vectorized Code](https://raw.githubusercontent.com/kks32-courses/accelerating-python/main/images/vectorized-code.png)\n",
        "\n",
        "\n",
        "![no-vectorization](https://raw.githubusercontent.com/kks32-courses/accelerating-python/main/images/novectorization.png)\n",
        "> Non-vectorized for-loop\n",
        "\n",
        "![vectorization](https://raw.githubusercontent.com/kks32-courses/accelerating-python/main/images/vectorized.png)\n",
        "> Vectorized iteration\n",
        "\n",
        "__Benefits of vectorization:__\n",
        "\n",
        "1. Performance: Vectorized operations can be significantly faster than their non-vectorized counterparts due to parallelism and efficient use of processor resources.\n",
        "\n",
        "1. Code readability: Vectorized code is often more concise and easier to read than non-vectorized code, as it eliminates the need for explicit loops and focuses on high-level operations.\n",
        "\n",
        "1. Portability: Vectorized code is more likely to benefit from performance improvements in hardware or software libraries, as it is designed to take advantage of SIMD instructions and parallelism.\n",
        "\n",
        "However, vectorization may not be suitable for all types of problems, particularly those with complex dependencies or irregular data structures. In such cases, other optimization techniques, like parallelization or algorithmic improvements, might be more appropriate."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorization in Numpy\n",
        "\n",
        "Numpy is a the fundamental package for scientific computing in Python. Although Numpy is a Python package, it was not developed in Python. Rather, it is written mostly in C and consists of binary executables compiled from source code. Numpy functions are therefore generally significantly faster than the same operations performed in Python. The term \"vectorized operation\" refers to passing an entire Numpy array of known data type to an optimized, compiled C code. The example below shows a simple calculation of a harmonic function using vectorized operations compared with the same operation in a Python loop. The vectorized calculation is much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMCQamM9e0i4",
        "outputId": "4c499c6d-62e7-4fb6-e716-998d7bebf92f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorized operation time: 0.09802 seconds\n",
            "Vectorized operation is 404.4 times faster\n"
          ]
        }
      ],
      "source": [
        "# Vectorized operation that passes time array into Numpy sin function\n",
        "vectorized_start_time = time.time()\n",
        "signal1 = np.sin(2.0 * np.pi * f * t)\n",
        "vectorized_end_time = time.time()\n",
        "print(\n",
        "    f\"Vectorized operation time: {vectorized_end_time - vectorized_start_time:.5f} seconds\"\n",
        ")\n",
        "# Ratio of execution times\n",
        "print(\n",
        "    f\"Vectorized operation is {(loop_end_time - loop_start_time) / (vectorized_end_time - vectorized_start_time):.1f} times faster\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y61E4cXLh7VT"
      },
      "source": [
        "## Larger-domain size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Js13JFIueyPf"
      },
      "outputs": [],
      "source": [
        "# Number of time steps\n",
        "N = 100000000\n",
        "\n",
        "# Numpy array containing time vector consisting of 32 bit floating point precision values\n",
        "t = np.linspace(0, N * dt, N, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpFY9FYOc5qJ",
        "outputId": "4dd98241-0643-4197-baf3-dc3274dec8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorized operation time: 1.13637 seconds\n"
          ]
        }
      ],
      "source": [
        "# Vectorized operation that passes time array into Numpy sin function\n",
        "vectorized_start_time = time.time()\n",
        "signal1 = np.sin(2.0 * np.pi * f * t)\n",
        "vectorized_end_time = time.time()\n",
        "print(\n",
        "    f\"Vectorized operation time: {vectorized_end_time - vectorized_start_time:.5f} seconds\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deaFGbiah9-F"
      },
      "source": [
        "## JAX\n",
        "\n",
        "JAX is a Python library developed by Google for optimized scientific computing:\n",
        "\n",
        "* JAX can be considered an alternative to NumPy, as it provides a very similar interface while also offering support for GPUs and TPUs. JAX includes `jax.numpy`, which closely mirrors the NumPy API, making it easy for developers to transition to JAX. Most operations that can be performed with NumPy can also be performed with `jax.numpy`.\n",
        "\n",
        "* JAX runs on accelerators (e.g., GPUs and TPUs) by leveraging Just-In-Time (JIT) compilation of both Python and JAX code with XLA (Accelerated Linear Algebra, a compiler) to generate optimized kernels. A kernel is a routine specifically compiled for high-throughput accelerators (e.g., GPUs and TPUs) that can be utilized by a main program. JIT compilation can be initiated using `jax.jit()`.\n",
        "\n",
        "* JAX offers robust support for automatic differentiation, which is particularly useful for machine learning research. Automatic differentiation can be activated with `jax.grad()`.\n",
        "\n",
        "* JAX promotes functional programming, as its functions are pure. Unlike NumPy arrays, JAX arrays are always immutable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZitikvPvdsih"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import time\n",
        "\n",
        "# JAX array containing time vector consisting of 32 bit floating point precision values\n",
        "t = jnp.linspace(0, N * dt, N, dtype=jnp.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwW_FysXjMWk",
        "outputId": "32d27186-2b42-418d-c4ca-0a2657cc3130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX CPU operation time: 0.31931 seconds\n"
          ]
        }
      ],
      "source": [
        "# Vectorized operation that passes time array into JAX sin function\n",
        "jax_start_time = time.time()\n",
        "signal1 = jnp.sin(2.0 * jnp.pi * f * t)\n",
        "jax_end_time = time.time()\n",
        "print(\n",
        "    f\"JAX CPU operation time: {jax_end_time - jax_start_time:.5f} seconds\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ4Xo1tpikqO"
      },
      "source": [
        "### Run on GPU/TPU node\n",
        "\n",
        "Runtime >> Change Runtime Type to `GPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkDmtSq4cMaU",
        "outputId": "e5f5b42d-7868-4a3d-fd78-9a36853fb06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX GPU operation time: 0.00056 seconds\n"
          ]
        }
      ],
      "source": [
        "# Vectorized operation that passes time array into JAX sin function\n",
        "jax_gpu_start_time = time.time()\n",
        "signal1 = jnp.sin(2.0 * jnp.pi * f * t)\n",
        "jax_gpu_end_time = time.time()\n",
        "print(\n",
        "    f\"JAX GPU operation time: {jax_gpu_end_time - jax_gpu_start_time:.5f} seconds\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
